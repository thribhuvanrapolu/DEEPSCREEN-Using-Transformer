{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uplpzWcPOfD6",
    "outputId": "5d45b8e0-d744-4e94-e9a2-d56431c3939d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: linformer in /dgxa_home/se21ucse231/anaconda3/lib/python3.11/site-packages (0.2.3)\n",
      "Requirement already satisfied: torch in /dgxa_home/se21ucse231/anaconda3/lib/python3.11/site-packages (from linformer) (2.3.0)\n",
      "Requirement already satisfied: filelock in /dgxa_home/se21ucse231/anaconda3/lib/python3.11/site-packages (from torch->linformer) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /dgxa_home/se21ucse231/anaconda3/lib/python3.11/site-packages (from torch->linformer) (4.9.0)\n",
      "Requirement already satisfied: sympy in /dgxa_home/se21ucse231/anaconda3/lib/python3.11/site-packages (from torch->linformer) (1.12)\n",
      "Requirement already satisfied: networkx in /dgxa_home/se21ucse231/anaconda3/lib/python3.11/site-packages (from torch->linformer) (3.1)\n",
      "Requirement already satisfied: jinja2 in /dgxa_home/se21ucse231/anaconda3/lib/python3.11/site-packages (from torch->linformer) (3.1.3)\n",
      "Requirement already satisfied: fsspec in /dgxa_home/se21ucse231/anaconda3/lib/python3.11/site-packages (from torch->linformer) (2023.10.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /dgxa_home/se21ucse231/anaconda3/lib/python3.11/site-packages (from jinja2->torch->linformer) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in /dgxa_home/se21ucse231/anaconda3/lib/python3.11/site-packages (from sympy->torch->linformer) (1.3.0)\n",
      "Requirement already satisfied: vit_pytorch in /dgxa_home/se21ucse231/anaconda3/lib/python3.11/site-packages (1.6.9)\n",
      "Requirement already satisfied: einops>=0.7.0 in /dgxa_home/se21ucse231/anaconda3/lib/python3.11/site-packages (from vit_pytorch) (0.8.0)\n",
      "Requirement already satisfied: torch>=1.10 in /dgxa_home/se21ucse231/anaconda3/lib/python3.11/site-packages (from vit_pytorch) (2.3.0)\n",
      "Requirement already satisfied: torchvision in /dgxa_home/se21ucse231/anaconda3/lib/python3.11/site-packages (from vit_pytorch) (0.18.0)\n",
      "Requirement already satisfied: filelock in /dgxa_home/se21ucse231/anaconda3/lib/python3.11/site-packages (from torch>=1.10->vit_pytorch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /dgxa_home/se21ucse231/anaconda3/lib/python3.11/site-packages (from torch>=1.10->vit_pytorch) (4.9.0)\n",
      "Requirement already satisfied: sympy in /dgxa_home/se21ucse231/anaconda3/lib/python3.11/site-packages (from torch>=1.10->vit_pytorch) (1.12)\n",
      "Requirement already satisfied: networkx in /dgxa_home/se21ucse231/anaconda3/lib/python3.11/site-packages (from torch>=1.10->vit_pytorch) (3.1)\n",
      "Requirement already satisfied: jinja2 in /dgxa_home/se21ucse231/anaconda3/lib/python3.11/site-packages (from torch>=1.10->vit_pytorch) (3.1.3)\n",
      "Requirement already satisfied: fsspec in /dgxa_home/se21ucse231/anaconda3/lib/python3.11/site-packages (from torch>=1.10->vit_pytorch) (2023.10.0)\n",
      "Requirement already satisfied: numpy in /dgxa_home/se21ucse231/anaconda3/lib/python3.11/site-packages (from torchvision->vit_pytorch) (1.26.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /dgxa_home/se21ucse231/anaconda3/lib/python3.11/site-packages (from torchvision->vit_pytorch) (10.2.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /dgxa_home/se21ucse231/anaconda3/lib/python3.11/site-packages (from jinja2->torch>=1.10->vit_pytorch) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in /dgxa_home/se21ucse231/anaconda3/lib/python3.11/site-packages (from sympy->torch>=1.10->vit_pytorch) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "# Install required packages\n",
    "!pip install linformer\n",
    "!pip install vit_pytorch\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from linformer import Linformer\n",
    "from PIL import Image\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from tqdm.notebook import tqdm\n",
    "from vit_pytorch.efficient import ViT\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import torch.utils.data as data\n",
    "import torchvision\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EEJVDV_mOfD7",
    "outputId": "e068c2cd-c101-48e5-cc2e-ccf761892dfd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fa3fbe0bb30>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check if CUDA is available\n",
    "print(torch.cuda.is_available())\n",
    "\n",
    "# Set device\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# Set seed for reproducibility\n",
    "torch.manual_seed(142)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "IRzTexQtOfD8"
   },
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "batch_size = 64\n",
    "epochs = 100\n",
    "lr = 0.0001\n",
    "gamma = 0.7\n",
    "IMG_SIZE = 200\n",
    "patch_size = 20\n",
    "num_classes = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "n-PNraU-OfD8"
   },
   "outputs": [],
   "source": [
    "# Transforms for image resizing and normalization\n",
    "'''transform = transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.ToTensor()\n",
    "])'''\n",
    "'''transform = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(IMG_SIZE),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(20),\n",
    "    transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3, hue=0.3),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])'''\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomVerticalFlip(),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2),\n",
    "    transforms.ToTensor()\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "7b2ecfae6fbc4edcbf7673125b5ee836",
      "2b0e82bbc5294756805dbe235a842ce0",
      "1a16729ad428466c88a5b618b7e37062",
      "e3fb1a1cb7a64e0ba2287a33f90930dd",
      "069fa16dcc484b4ebed48cae6f7f8cbe",
      "54b2a83b4f394e3bb6026b15b8def091",
      "b6df6d984d1c4c1689a899bd4e65bc8d",
      "2949f13aeca94eda8555827eadc52fae",
      "44a5ad178bfc4eee9e68535fd5bf5f8f",
      "7f0f91e9b1b44aae90a8001c22ce0553",
      "3ab3973edc4344deb89c236f119ee834"
     ]
    },
    "id": "BhTtUlhVOfD8",
    "outputId": "798c8625-ffc7-4527-949a-cfae66e20e16"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c69ff5811ab140d9bca5b85934df2118",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, Loss: 0.8855, Accuracy: 0.5063, Val Loss: 0.6849, Val Accuracy: 0.5878\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b3deb32c67b4ceca8ccc84d142a00fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100, Loss: 0.6736, Accuracy: 0.6023, Val Loss: 0.6809, Val Accuracy: 0.5913\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b49da6fc16e34a07824ac6cd66abaf71",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/100, Loss: 0.6742, Accuracy: 0.5996, Val Loss: 0.6778, Val Accuracy: 0.5948\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb3d9b5956bd4c9ea680636c6989de23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/100, Loss: 0.6711, Accuracy: 0.5979, Val Loss: 0.6715, Val Accuracy: 0.6001\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f7cdee21a234872bfa7ceb20913c41e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/100, Loss: 0.6691, Accuracy: 0.5979, Val Loss: 0.6622, Val Accuracy: 0.6501\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "470b87472ad8459c8817c54ab2ce7b74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/100, Loss: 0.6544, Accuracy: 0.6196, Val Loss: 0.6527, Val Accuracy: 0.6345\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0a293d7ac3444768192ec588bab1422",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/100, Loss: 0.6441, Accuracy: 0.6408, Val Loss: 0.6614, Val Accuracy: 0.6585\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9810399c4a114b4f97a5c6d092205721",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/100, Loss: 0.6142, Accuracy: 0.6739, Val Loss: 0.6404, Val Accuracy: 0.6307\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4eea24c089048dda7ca5f2c05a4f243",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/100, Loss: 0.6023, Accuracy: 0.6943, Val Loss: 0.6452, Val Accuracy: 0.6386\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "edda030a0f644aa19ef12b6f936e39c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/100, Loss: 0.5911, Accuracy: 0.6877, Val Loss: 0.6436, Val Accuracy: 0.6241\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c566ad5853e48b185ae57b3a78ae812",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/100, Loss: 0.5804, Accuracy: 0.7046, Val Loss: 0.6213, Val Accuracy: 0.6745\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f171211ca1a14ff794b431b976b890a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/100, Loss: 0.5504, Accuracy: 0.7257, Val Loss: 0.5967, Val Accuracy: 0.6800\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f1517eee40d4292a70f901bd3a6fc8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/100, Loss: 0.5393, Accuracy: 0.7404, Val Loss: 0.6341, Val Accuracy: 0.6727\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07aa271509624f5a97695b23e7c2caa2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/100, Loss: 0.5179, Accuracy: 0.7412, Val Loss: 0.6357, Val Accuracy: 0.6776\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ed7213f02bf40789d1b2d785b549d15",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/100, Loss: 0.4870, Accuracy: 0.7858, Val Loss: 0.5969, Val Accuracy: 0.6970\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "786a14d9fbda4a38a404e1821ce07a17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/100, Loss: 0.4608, Accuracy: 0.7939, Val Loss: 0.5564, Val Accuracy: 0.7422\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5fd9fdf6a7aa467facbb3d8310201962",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/100, Loss: 0.4286, Accuracy: 0.8016, Val Loss: 0.6341, Val Accuracy: 0.6727\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ccfece14afbe4e5ea2e5560abd6b9496",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/100, Loss: 0.4105, Accuracy: 0.8114, Val Loss: 0.6516, Val Accuracy: 0.6928\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd4f6cd98e074bce807885546d22eece",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/100, Loss: 0.3745, Accuracy: 0.8393, Val Loss: 0.6123, Val Accuracy: 0.7085\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd528929609d464ea8960859352b0513",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/100, Loss: 0.3625, Accuracy: 0.8427, Val Loss: 0.6841, Val Accuracy: 0.7085\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aadaab92abcf4bd28a216ba47a0587f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21/100, Loss: 0.3673, Accuracy: 0.8483, Val Loss: 0.7121, Val Accuracy: 0.6553\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c4605c2451b4289ab094589303ae056",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22/100, Loss: 0.3177, Accuracy: 0.8680, Val Loss: 0.7095, Val Accuracy: 0.7210\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e33e49f4f82452dab1fac9bd890645b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/100, Loss: 0.2862, Accuracy: 0.8832, Val Loss: 0.7353, Val Accuracy: 0.6970\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2331bca25b9245ec99caefa6f878e277",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/100, Loss: 0.2601, Accuracy: 0.8901, Val Loss: 0.7771, Val Accuracy: 0.6956\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03e1322e8e8348e099f6ba1d8639e956",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/100, Loss: 0.2787, Accuracy: 0.8905, Val Loss: 0.7199, Val Accuracy: 0.7227\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a07ba59ed71643ec88c9603d9d9f4395",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26/100, Loss: 0.2708, Accuracy: 0.8987, Val Loss: 0.7350, Val Accuracy: 0.6868\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd1ad1ce8bac41428e3ebd03badd8939",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/100, Loss: 0.2261, Accuracy: 0.9135, Val Loss: 0.8822, Val Accuracy: 0.6893\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff6dd8e412744824ab720b34b945ba34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28/100, Loss: 0.2220, Accuracy: 0.9168, Val Loss: 0.7347, Val Accuracy: 0.7063\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "045aea18a46e444a99fdbd657bd836a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29/100, Loss: 0.2111, Accuracy: 0.9177, Val Loss: 0.7894, Val Accuracy: 0.7022\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78953c06053a4487b608b9f718dded77",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30/100, Loss: 0.1919, Accuracy: 0.9341, Val Loss: 0.8801, Val Accuracy: 0.6803\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab04be5622164f688571fd78edc469f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31/100, Loss: 0.1800, Accuracy: 0.9284, Val Loss: 0.7658, Val Accuracy: 0.6905\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8158d91d8c2646c98cc299ca55f969ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32/100, Loss: 0.1357, Accuracy: 0.9522, Val Loss: 1.1183, Val Accuracy: 0.6995\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "215be7d0935d4e8c8d605ee7288c0811",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33/100, Loss: 0.1237, Accuracy: 0.9481, Val Loss: 0.9919, Val Accuracy: 0.7227\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d72d4a319b9847409b52357299555a83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 34/100, Loss: 0.1521, Accuracy: 0.9473, Val Loss: 0.8475, Val Accuracy: 0.7530\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "930c3dfe3e92495c934e70e8e6be1737",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 35/100, Loss: 0.1150, Accuracy: 0.9579, Val Loss: 0.8856, Val Accuracy: 0.7745\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca99a6599ffc4cfdafcabeb984c8f554",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 36/100, Loss: 0.1068, Accuracy: 0.9588, Val Loss: 1.0477, Val Accuracy: 0.7227\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37fe6406ecac4b1fb359269ca37c3e1d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 37/100, Loss: 0.0946, Accuracy: 0.9696, Val Loss: 0.9141, Val Accuracy: 0.7147\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92ef449d2d244a01aa71377349cc7a19",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 38/100, Loss: 0.0918, Accuracy: 0.9687, Val Loss: 0.9542, Val Accuracy: 0.7202\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7bc949b929a43dbb97449293c6f9f02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 39/100, Loss: 0.0769, Accuracy: 0.9745, Val Loss: 1.1553, Val Accuracy: 0.7335\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d916e220bec46d0a1ead53238bd9121",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40/100, Loss: 0.0801, Accuracy: 0.9695, Val Loss: 1.0263, Val Accuracy: 0.7739\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1005a3b69f124513aa1917fa770250a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 41/100, Loss: 0.0700, Accuracy: 0.9794, Val Loss: 1.1983, Val Accuracy: 0.7217\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87fbd3160fd943b8b3a87b259da8bc24",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 42/100, Loss: 0.0690, Accuracy: 0.9786, Val Loss: 1.1625, Val Accuracy: 0.7001\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e3d4ec5b020436f84d1cc36c09859b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 43/100, Loss: 0.0675, Accuracy: 0.9794, Val Loss: 1.2111, Val Accuracy: 0.7178\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cdda177e7d4746db8b7e3f0fa42e163c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 44/100, Loss: 0.0729, Accuracy: 0.9712, Val Loss: 1.1931, Val Accuracy: 0.7079\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26342a25dec746d58001ff70578fe65f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 45/100, Loss: 0.0490, Accuracy: 0.9786, Val Loss: 1.4385, Val Accuracy: 0.6768\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97c63985d62745568701852a7407f3ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 46/100, Loss: 0.0725, Accuracy: 0.9786, Val Loss: 1.2164, Val Accuracy: 0.7380\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33721e6184c942afb4959c104e4959ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 47/100, Loss: 0.0528, Accuracy: 0.9810, Val Loss: 1.2816, Val Accuracy: 0.7143\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f91201f07eef4546baa67cf702729614",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 48/100, Loss: 0.0717, Accuracy: 0.9778, Val Loss: 0.9952, Val Accuracy: 0.7418\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56d3624f61914a91a7e7f9f5fc24fd71",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49/100, Loss: 0.0670, Accuracy: 0.9753, Val Loss: 1.1870, Val Accuracy: 0.7088\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7432cb05165447c90762a48ba26045b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/100, Loss: 0.0484, Accuracy: 0.9802, Val Loss: 1.2624, Val Accuracy: 0.7171\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82420a362b584fb7ad45e42821059045",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 51/100, Loss: 0.0654, Accuracy: 0.9778, Val Loss: 1.0385, Val Accuracy: 0.7321\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3595db0db60d4690930279f5c2a85a3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 52/100, Loss: 0.0485, Accuracy: 0.9836, Val Loss: 1.0292, Val Accuracy: 0.7130\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ecbd3ee87bcf4a8fa25d7e51c11e8b84",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 53/100, Loss: 0.0348, Accuracy: 0.9893, Val Loss: 1.4026, Val Accuracy: 0.7223\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd01b4ea08674c2887d0da2cabc8783c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 54/100, Loss: 0.0380, Accuracy: 0.9827, Val Loss: 1.2678, Val Accuracy: 0.7428\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c24585c154ab4fbfb39f1696fac8c6bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 55/100, Loss: 0.0326, Accuracy: 0.9910, Val Loss: 1.2343, Val Accuracy: 0.7411\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02dba7ef5b75497ca2f72d90510a91cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 56/100, Loss: 0.0617, Accuracy: 0.9770, Val Loss: 1.0464, Val Accuracy: 0.7422\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "577c7f0c46f64b56b2ed8cf866946357",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 57/100, Loss: 0.0485, Accuracy: 0.9819, Val Loss: 1.3427, Val Accuracy: 0.6925\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc5d2c3f31fc4ffcbfb6c50f1ede7108",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 58/100, Loss: 0.0368, Accuracy: 0.9868, Val Loss: 1.0382, Val Accuracy: 0.7498\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8f2bc3f10ef4be58b16273927998bd6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 59/100, Loss: 0.0449, Accuracy: 0.9819, Val Loss: 1.1635, Val Accuracy: 0.7245\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13aae1b9398b4827a778a05c4536b792",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 60/100, Loss: 0.0202, Accuracy: 0.9959, Val Loss: 1.4032, Val Accuracy: 0.7217\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a1e2aa80c8447dfa6b7814924469a25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 61/100, Loss: 0.0429, Accuracy: 0.9844, Val Loss: 1.2820, Val Accuracy: 0.7290\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7070fbaa12ba4dd49e400d04c50af543",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 62/100, Loss: 0.0209, Accuracy: 0.9942, Val Loss: 1.2013, Val Accuracy: 0.7428\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "311e907f797e41cbacd38279981ea56d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 63/100, Loss: 0.0261, Accuracy: 0.9934, Val Loss: 1.6703, Val Accuracy: 0.6876\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3be8ee1f1d1e440cb12289f5850a523c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 64/100, Loss: 0.0213, Accuracy: 0.9918, Val Loss: 1.5006, Val Accuracy: 0.7397\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c7478825ff0403885aa48dd51bd4185",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 65/100, Loss: 0.0255, Accuracy: 0.9901, Val Loss: 1.5528, Val Accuracy: 0.7227\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4223639f9f0421d86dd52d497709cf0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 66/100, Loss: 0.0334, Accuracy: 0.9893, Val Loss: 1.3344, Val Accuracy: 0.7307\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8b3e6e0adc040dd95254d7bb0e9fd02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 67/100, Loss: 0.0235, Accuracy: 0.9934, Val Loss: 1.2168, Val Accuracy: 0.7352\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2abb44f7bbf40b690d79f5c6b1859ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 68/100, Loss: 0.0102, Accuracy: 0.9967, Val Loss: 1.4311, Val Accuracy: 0.7300\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a51eddd4c0c44ab4909e66bca5514b70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 69/100, Loss: 0.0280, Accuracy: 0.9918, Val Loss: 1.2584, Val Accuracy: 0.7565\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a1b8fbd12334e8c89045d445fe3955f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 70/100, Loss: 0.0319, Accuracy: 0.9885, Val Loss: 1.3538, Val Accuracy: 0.7141\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5ca9c52dbd940feb7e958c52a84579c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 71/100, Loss: 0.0252, Accuracy: 0.9910, Val Loss: 1.5692, Val Accuracy: 0.7116\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb60d6b0a5ac4663a34952b6321e480f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 72/100, Loss: 0.0253, Accuracy: 0.9909, Val Loss: 1.3583, Val Accuracy: 0.7067\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9ec91f7303d4b1f971c83a5dec6aae7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 73/100, Loss: 0.0226, Accuracy: 0.9901, Val Loss: 1.4313, Val Accuracy: 0.7255\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6635f9fa34ff458e9e87effb04d0ca52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 74/100, Loss: 0.0271, Accuracy: 0.9909, Val Loss: 1.4540, Val Accuracy: 0.7352\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26e5c58b1d4d4235b0aa0b87c3203452",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75/100, Loss: 0.0187, Accuracy: 0.9951, Val Loss: 1.5186, Val Accuracy: 0.7231\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ea4606e5322459582e1f8863dd1b6e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 76/100, Loss: 0.0242, Accuracy: 0.9934, Val Loss: 1.2111, Val Accuracy: 0.7561\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d21812900acb4ebcbe7872ca2c57ab8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 77/100, Loss: 0.0207, Accuracy: 0.9934, Val Loss: 1.4165, Val Accuracy: 0.7293\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "977a0bbb5cde4c6f9922a01a9fe004c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 78/100, Loss: 0.0342, Accuracy: 0.9868, Val Loss: 1.5540, Val Accuracy: 0.6934\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e31aefcae6174e189f09cc7175453608",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 79/100, Loss: 0.0397, Accuracy: 0.9885, Val Loss: 1.1613, Val Accuracy: 0.7248\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a94e4fcc8d3d418bae0c5821e720577b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 80/100, Loss: 0.0237, Accuracy: 0.9934, Val Loss: 1.2811, Val Accuracy: 0.7405\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39cb570dafac4e7dbbc31c8f1300143e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 81/100, Loss: 0.0204, Accuracy: 0.9926, Val Loss: 1.3760, Val Accuracy: 0.7311\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e54da3f777b64676852182211900ef11",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 82/100, Loss: 0.0174, Accuracy: 0.9951, Val Loss: 1.5276, Val Accuracy: 0.7088\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6835d2c79d7b466db344719431e62832",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 83/100, Loss: 0.0187, Accuracy: 0.9951, Val Loss: 1.3543, Val Accuracy: 0.7418\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aeb57b36f62240308e8f0b84b79a1914",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 84/100, Loss: 0.0130, Accuracy: 0.9975, Val Loss: 1.4646, Val Accuracy: 0.7303\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09e56672e478450dbc3c52ae3f2f6ecf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 85/100, Loss: 0.0053, Accuracy: 0.9984, Val Loss: 1.3651, Val Accuracy: 0.7428\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "856f0b5d5d58401a9632c3c3e623d5b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 86/100, Loss: 0.0124, Accuracy: 0.9967, Val Loss: 1.4649, Val Accuracy: 0.7512\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25bf381913ff419e96e94d2d0cc6fc6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 87/100, Loss: 0.0172, Accuracy: 0.9967, Val Loss: 1.5958, Val Accuracy: 0.7428\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0c8d8a94e5247e9abdf322dd063c54c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 88/100, Loss: 0.0136, Accuracy: 0.9942, Val Loss: 1.5323, Val Accuracy: 0.7245\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f605a243f6544f5db28bf53418ec94b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 89/100, Loss: 0.0144, Accuracy: 0.9951, Val Loss: 1.3879, Val Accuracy: 0.7473\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3194ce8bbb4a4f50959f0bb57269518a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 90/100, Loss: 0.0176, Accuracy: 0.9950, Val Loss: 1.3035, Val Accuracy: 0.7571\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81fd180c51e84dc3a807c3eecc843eff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 91/100, Loss: 0.0134, Accuracy: 0.9959, Val Loss: 1.5051, Val Accuracy: 0.7213\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6cb2a8f8b334460c9624294e67bbb1c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 92/100, Loss: 0.0272, Accuracy: 0.9934, Val Loss: 1.3253, Val Accuracy: 0.7547\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "348b4dc4696d49ac8702942b47ac4b6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 93/100, Loss: 0.0163, Accuracy: 0.9934, Val Loss: 1.1967, Val Accuracy: 0.7686\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23bb36106ba64c368d2a7624ac7bb935",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 94/100, Loss: 0.0245, Accuracy: 0.9926, Val Loss: 1.3074, Val Accuracy: 0.7516\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d78650d1b5104bc8a558ca012a9d68e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 95/100, Loss: 0.0124, Accuracy: 0.9967, Val Loss: 1.2428, Val Accuracy: 0.7356\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a7cdbe24e544fdb8886ba4cc58dc8e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 96/100, Loss: 0.0149, Accuracy: 0.9951, Val Loss: 1.1836, Val Accuracy: 0.7512\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36c96e9879e844da9de627ca7dd84c44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 97/100, Loss: 0.0155, Accuracy: 0.9942, Val Loss: 1.3483, Val Accuracy: 0.7409\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ee2c46853a4481781e4628cf11211a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 98/100, Loss: 0.0084, Accuracy: 0.9975, Val Loss: 1.4181, Val Accuracy: 0.7272\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9519ae9f4d43465a9b4a9e5f82970a84",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 99/100, Loss: 0.0136, Accuracy: 0.9958, Val Loss: 1.4238, Val Accuracy: 0.7227\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21ceadc4b7014f788724be61186ad57a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100/100, Loss: 0.0176, Accuracy: 0.9942, Val Loss: 1.5795, Val Accuracy: 0.7241\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for ViT:\n\tUnexpected key(s) in state_dict: \"transformer.net.layers.12.0.fn.proj_k\", \"transformer.net.layers.12.0.fn.proj_v\", \"transformer.net.layers.12.0.fn.to_q.weight\", \"transformer.net.layers.12.0.fn.to_k.weight\", \"transformer.net.layers.12.0.fn.to_v.weight\", \"transformer.net.layers.12.0.fn.to_out.weight\", \"transformer.net.layers.12.0.fn.to_out.bias\", \"transformer.net.layers.12.0.norm.weight\", \"transformer.net.layers.12.0.norm.bias\", \"transformer.net.layers.12.1.fn.w1.weight\", \"transformer.net.layers.12.1.fn.w1.bias\", \"transformer.net.layers.12.1.fn.w2.weight\", \"transformer.net.layers.12.1.fn.w2.bias\", \"transformer.net.layers.12.1.norm.weight\", \"transformer.net.layers.12.1.norm.bias\", \"transformer.net.layers.13.0.fn.proj_k\", \"transformer.net.layers.13.0.fn.proj_v\", \"transformer.net.layers.13.0.fn.to_q.weight\", \"transformer.net.layers.13.0.fn.to_k.weight\", \"transformer.net.layers.13.0.fn.to_v.weight\", \"transformer.net.layers.13.0.fn.to_out.weight\", \"transformer.net.layers.13.0.fn.to_out.bias\", \"transformer.net.layers.13.0.norm.weight\", \"transformer.net.layers.13.0.norm.bias\", \"transformer.net.layers.13.1.fn.w1.weight\", \"transformer.net.layers.13.1.fn.w1.bias\", \"transformer.net.layers.13.1.fn.w2.weight\", \"transformer.net.layers.13.1.fn.w2.bias\", \"transformer.net.layers.13.1.norm.weight\", \"transformer.net.layers.13.1.norm.bias\", \"transformer.net.layers.14.0.fn.proj_k\", \"transformer.net.layers.14.0.fn.proj_v\", \"transformer.net.layers.14.0.fn.to_q.weight\", \"transformer.net.layers.14.0.fn.to_k.weight\", \"transformer.net.layers.14.0.fn.to_v.weight\", \"transformer.net.layers.14.0.fn.to_out.weight\", \"transformer.net.layers.14.0.fn.to_out.bias\", \"transformer.net.layers.14.0.norm.weight\", \"transformer.net.layers.14.0.norm.bias\", \"transformer.net.layers.14.1.fn.w1.weight\", \"transformer.net.layers.14.1.fn.w1.bias\", \"transformer.net.layers.14.1.fn.w2.weight\", \"transformer.net.layers.14.1.fn.w2.bias\", \"transformer.net.layers.14.1.norm.weight\", \"transformer.net.layers.14.1.norm.bias\", \"transformer.net.layers.15.0.fn.proj_k\", \"transformer.net.layers.15.0.fn.proj_v\", \"transformer.net.layers.15.0.fn.to_q.weight\", \"transformer.net.layers.15.0.fn.to_k.weight\", \"transformer.net.layers.15.0.fn.to_v.weight\", \"transformer.net.layers.15.0.fn.to_out.weight\", \"transformer.net.layers.15.0.fn.to_out.bias\", \"transformer.net.layers.15.0.norm.weight\", \"transformer.net.layers.15.0.norm.bias\", \"transformer.net.layers.15.1.fn.w1.weight\", \"transformer.net.layers.15.1.fn.w1.bias\", \"transformer.net.layers.15.1.fn.w2.weight\", \"transformer.net.layers.15.1.fn.w2.bias\", \"transformer.net.layers.15.1.norm.weight\", \"transformer.net.layers.15.1.norm.bias\", \"transformer.net.layers.16.0.fn.proj_k\", \"transformer.net.layers.16.0.fn.proj_v\", \"transformer.net.layers.16.0.fn.to_q.weight\", \"transformer.net.layers.16.0.fn.to_k.weight\", \"transformer.net.layers.16.0.fn.to_v.weight\", \"transformer.net.layers.16.0.fn.to_out.weight\", \"transformer.net.layers.16.0.fn.to_out.bias\", \"transformer.net.layers.16.0.norm.weight\", \"transformer.net.layers.16.0.norm.bias\", \"transformer.net.layers.16.1.fn.w1.weight\", \"transformer.net.layers.16.1.fn.w1.bias\", \"transformer.net.layers.16.1.fn.w2.weight\", \"transformer.net.layers.16.1.fn.w2.bias\", \"transformer.net.layers.16.1.norm.weight\", \"transformer.net.layers.16.1.norm.bias\", \"transformer.net.layers.17.0.fn.proj_k\", \"transformer.net.layers.17.0.fn.proj_v\", \"transformer.net.layers.17.0.fn.to_q.weight\", \"transformer.net.layers.17.0.fn.to_k.weight\", \"transformer.net.layers.17.0.fn.to_v.weight\", \"transformer.net.layers.17.0.fn.to_out.weight\", \"transformer.net.layers.17.0.fn.to_out.bias\", \"transformer.net.layers.17.0.norm.weight\", \"transformer.net.layers.17.0.norm.bias\", \"transformer.net.layers.17.1.fn.w1.weight\", \"transformer.net.layers.17.1.fn.w1.bias\", \"transformer.net.layers.17.1.fn.w2.weight\", \"transformer.net.layers.17.1.fn.w2.bias\", \"transformer.net.layers.17.1.norm.weight\", \"transformer.net.layers.17.1.norm.bias\", \"transformer.net.layers.18.0.fn.proj_k\", \"transformer.net.layers.18.0.fn.proj_v\", \"transformer.net.layers.18.0.fn.to_q.weight\", \"transformer.net.layers.18.0.fn.to_k.weight\", \"transformer.net.layers.18.0.fn.to_v.weight\", \"transformer.net.layers.18.0.fn.to_out.weight\", \"transformer.net.layers.18.0.fn.to_out.bias\", \"transformer.net.layers.18.0.norm.weight\", \"transformer.net.layers.18.0.norm.bias\", \"transformer.net.layers.18.1.fn.w1.weight\", \"transformer.net.layers.18.1.fn.w1.bias\", \"transformer.net.layers.18.1.fn.w2.weight\", \"transformer.net.layers.18.1.fn.w2.bias\", \"transformer.net.layers.18.1.norm.weight\", \"transformer.net.layers.18.1.norm.bias\", \"transformer.net.layers.19.0.fn.proj_k\", \"transformer.net.layers.19.0.fn.proj_v\", \"transformer.net.layers.19.0.fn.to_q.weight\", \"transformer.net.layers.19.0.fn.to_k.weight\", \"transformer.net.layers.19.0.fn.to_v.weight\", \"transformer.net.layers.19.0.fn.to_out.weight\", \"transformer.net.layers.19.0.fn.to_out.bias\", \"transformer.net.layers.19.0.norm.weight\", \"transformer.net.layers.19.0.norm.bias\", \"transformer.net.layers.19.1.fn.w1.weight\", \"transformer.net.layers.19.1.fn.w1.bias\", \"transformer.net.layers.19.1.fn.w2.weight\", \"transformer.net.layers.19.1.fn.w2.bias\", \"transformer.net.layers.19.1.norm.weight\", \"transformer.net.layers.19.1.norm.bias\", \"transformer.net.layers.20.0.fn.proj_k\", \"transformer.net.layers.20.0.fn.proj_v\", \"transformer.net.layers.20.0.fn.to_q.weight\", \"transformer.net.layers.20.0.fn.to_k.weight\", \"transformer.net.layers.20.0.fn.to_v.weight\", \"transformer.net.layers.20.0.fn.to_out.weight\", \"transformer.net.layers.20.0.fn.to_out.bias\", \"transformer.net.layers.20.0.norm.weight\", \"transformer.net.layers.20.0.norm.bias\", \"transformer.net.layers.20.1.fn.w1.weight\", \"transformer.net.layers.20.1.fn.w1.bias\", \"transformer.net.layers.20.1.fn.w2.weight\", \"transformer.net.layers.20.1.fn.w2.bias\", \"transformer.net.layers.20.1.norm.weight\", \"transformer.net.layers.20.1.norm.bias\", \"transformer.net.layers.21.0.fn.proj_k\", \"transformer.net.layers.21.0.fn.proj_v\", \"transformer.net.layers.21.0.fn.to_q.weight\", \"transformer.net.layers.21.0.fn.to_k.weight\", \"transformer.net.layers.21.0.fn.to_v.weight\", \"transformer.net.layers.21.0.fn.to_out.weight\", \"transformer.net.layers.21.0.fn.to_out.bias\", \"transformer.net.layers.21.0.norm.weight\", \"transformer.net.layers.21.0.norm.bias\", \"transformer.net.layers.21.1.fn.w1.weight\", \"transformer.net.layers.21.1.fn.w1.bias\", \"transformer.net.layers.21.1.fn.w2.weight\", \"transformer.net.layers.21.1.fn.w2.bias\", \"transformer.net.layers.21.1.norm.weight\", \"transformer.net.layers.21.1.norm.bias\", \"transformer.net.layers.22.0.fn.proj_k\", \"transformer.net.layers.22.0.fn.proj_v\", \"transformer.net.layers.22.0.fn.to_q.weight\", \"transformer.net.layers.22.0.fn.to_k.weight\", \"transformer.net.layers.22.0.fn.to_v.weight\", \"transformer.net.layers.22.0.fn.to_out.weight\", \"transformer.net.layers.22.0.fn.to_out.bias\", \"transformer.net.layers.22.0.norm.weight\", \"transformer.net.layers.22.0.norm.bias\", \"transformer.net.layers.22.1.fn.w1.weight\", \"transformer.net.layers.22.1.fn.w1.bias\", \"transformer.net.layers.22.1.fn.w2.weight\", \"transformer.net.layers.22.1.fn.w2.bias\", \"transformer.net.layers.22.1.norm.weight\", \"transformer.net.layers.22.1.norm.bias\", \"transformer.net.layers.23.0.fn.proj_k\", \"transformer.net.layers.23.0.fn.proj_v\", \"transformer.net.layers.23.0.fn.to_q.weight\", \"transformer.net.layers.23.0.fn.to_k.weight\", \"transformer.net.layers.23.0.fn.to_v.weight\", \"transformer.net.layers.23.0.fn.to_out.weight\", \"transformer.net.layers.23.0.fn.to_out.bias\", \"transformer.net.layers.23.0.norm.weight\", \"transformer.net.layers.23.0.norm.bias\", \"transformer.net.layers.23.1.fn.w1.weight\", \"transformer.net.layers.23.1.fn.w1.bias\", \"transformer.net.layers.23.1.fn.w2.weight\", \"transformer.net.layers.23.1.fn.w2.bias\", \"transformer.net.layers.23.1.norm.weight\", \"transformer.net.layers.23.1.norm.bias\". \n\tsize mismatch for pos_embedding: copying a param with shape torch.Size([1, 101, 256]) from checkpoint, the shape in current model is torch.Size([1, 101, 128]).\n\tsize mismatch for cls_token: copying a param with shape torch.Size([1, 1, 256]) from checkpoint, the shape in current model is torch.Size([1, 1, 128]).\n\tsize mismatch for to_patch_embedding.2.weight: copying a param with shape torch.Size([256, 1200]) from checkpoint, the shape in current model is torch.Size([128, 1200]).\n\tsize mismatch for to_patch_embedding.2.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for to_patch_embedding.3.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for to_patch_embedding.3.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for transformer.net.layers.0.0.fn.proj_k: copying a param with shape torch.Size([101, 128]) from checkpoint, the shape in current model is torch.Size([101, 64]).\n\tsize mismatch for transformer.net.layers.0.0.fn.proj_v: copying a param with shape torch.Size([101, 128]) from checkpoint, the shape in current model is torch.Size([101, 64]).\n\tsize mismatch for transformer.net.layers.0.0.fn.to_q.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for transformer.net.layers.0.0.fn.to_k.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for transformer.net.layers.0.0.fn.to_v.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for transformer.net.layers.0.0.fn.to_out.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for transformer.net.layers.0.0.fn.to_out.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for transformer.net.layers.0.0.norm.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for transformer.net.layers.0.0.norm.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for transformer.net.layers.0.1.fn.w1.weight: copying a param with shape torch.Size([1024, 256]) from checkpoint, the shape in current model is torch.Size([512, 128]).\n\tsize mismatch for transformer.net.layers.0.1.fn.w1.bias: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for transformer.net.layers.0.1.fn.w2.weight: copying a param with shape torch.Size([256, 1024]) from checkpoint, the shape in current model is torch.Size([128, 512]).\n\tsize mismatch for transformer.net.layers.0.1.fn.w2.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for transformer.net.layers.0.1.norm.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for transformer.net.layers.0.1.norm.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for transformer.net.layers.1.0.fn.proj_k: copying a param with shape torch.Size([101, 128]) from checkpoint, the shape in current model is torch.Size([101, 64]).\n\tsize mismatch for transformer.net.layers.1.0.fn.proj_v: copying a param with shape torch.Size([101, 128]) from checkpoint, the shape in current model is torch.Size([101, 64]).\n\tsize mismatch for transformer.net.layers.1.0.fn.to_q.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for transformer.net.layers.1.0.fn.to_k.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for transformer.net.layers.1.0.fn.to_v.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for transformer.net.layers.1.0.fn.to_out.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for transformer.net.layers.1.0.fn.to_out.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for transformer.net.layers.1.0.norm.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for transformer.net.layers.1.0.norm.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for transformer.net.layers.1.1.fn.w1.weight: copying a param with shape torch.Size([1024, 256]) from checkpoint, the shape in current model is torch.Size([512, 128]).\n\tsize mismatch for transformer.net.layers.1.1.fn.w1.bias: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for transformer.net.layers.1.1.fn.w2.weight: copying a param with shape torch.Size([256, 1024]) from checkpoint, the shape in current model is torch.Size([128, 512]).\n\tsize mismatch for transformer.net.layers.1.1.fn.w2.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for transformer.net.layers.1.1.norm.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for transformer.net.layers.1.1.norm.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for transformer.net.layers.2.0.fn.proj_k: copying a param with shape torch.Size([101, 128]) from checkpoint, the shape in current model is torch.Size([101, 64]).\n\tsize mismatch for transformer.net.layers.2.0.fn.proj_v: copying a param with shape torch.Size([101, 128]) from checkpoint, the shape in current model is torch.Size([101, 64]).\n\tsize mismatch for transformer.net.layers.2.0.fn.to_q.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for transformer.net.layers.2.0.fn.to_k.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for transformer.net.layers.2.0.fn.to_v.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for transformer.net.layers.2.0.fn.to_out.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for transformer.net.layers.2.0.fn.to_out.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for transformer.net.layers.2.0.norm.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for transformer.net.layers.2.0.norm.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for transformer.net.layers.2.1.fn.w1.weight: copying a param with shape torch.Size([1024, 256]) from checkpoint, the shape in current model is torch.Size([512, 128]).\n\tsize mismatch for transformer.net.layers.2.1.fn.w1.bias: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for transformer.net.layers.2.1.fn.w2.weight: copying a param with shape torch.Size([256, 1024]) from checkpoint, the shape in current model is torch.Size([128, 512]).\n\tsize mismatch for transformer.net.layers.2.1.fn.w2.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for transformer.net.layers.2.1.norm.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for transformer.net.layers.2.1.norm.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for transformer.net.layers.3.0.fn.proj_k: copying a param with shape torch.Size([101, 128]) from checkpoint, the shape in current model is torch.Size([101, 64]).\n\tsize mismatch for transformer.net.layers.3.0.fn.proj_v: copying a param with shape torch.Size([101, 128]) from checkpoint, the shape in current model is torch.Size([101, 64]).\n\tsize mismatch for transformer.net.layers.3.0.fn.to_q.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for transformer.net.layers.3.0.fn.to_k.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for transformer.net.layers.3.0.fn.to_v.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for transformer.net.layers.3.0.fn.to_out.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for transformer.net.layers.3.0.fn.to_out.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for transformer.net.layers.3.0.norm.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for transformer.net.layers.3.0.norm.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for transformer.net.layers.3.1.fn.w1.weight: copying a param with shape torch.Size([1024, 256]) from checkpoint, the shape in current model is torch.Size([512, 128]).\n\tsize mismatch for transformer.net.layers.3.1.fn.w1.bias: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for transformer.net.layers.3.1.fn.w2.weight: copying a param with shape torch.Size([256, 1024]) from checkpoint, the shape in current model is torch.Size([128, 512]).\n\tsize mismatch for transformer.net.layers.3.1.fn.w2.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for transformer.net.layers.3.1.norm.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for transformer.net.layers.3.1.norm.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for transformer.net.layers.4.0.fn.proj_k: copying a param with shape torch.Size([101, 128]) from checkpoint, the shape in current model is torch.Size([101, 64]).\n\tsize mismatch for transformer.net.layers.4.0.fn.proj_v: copying a param with shape torch.Size([101, 128]) from checkpoint, the shape in current model is torch.Size([101, 64]).\n\tsize mismatch for transformer.net.layers.4.0.fn.to_q.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for transformer.net.layers.4.0.fn.to_k.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for transformer.net.layers.4.0.fn.to_v.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for transformer.net.layers.4.0.fn.to_out.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for transformer.net.layers.4.0.fn.to_out.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for transformer.net.layers.4.0.norm.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for transformer.net.layers.4.0.norm.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for transformer.net.layers.4.1.fn.w1.weight: copying a param with shape torch.Size([1024, 256]) from checkpoint, the shape in current model is torch.Size([512, 128]).\n\tsize mismatch for transformer.net.layers.4.1.fn.w1.bias: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for transformer.net.layers.4.1.fn.w2.weight: copying a param with shape torch.Size([256, 1024]) from checkpoint, the shape in current model is torch.Size([128, 512]).\n\tsize mismatch for transformer.net.layers.4.1.fn.w2.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for transformer.net.layers.4.1.norm.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for transformer.net.layers.4.1.norm.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for transformer.net.layers.5.0.fn.proj_k: copying a param with shape torch.Size([101, 128]) from checkpoint, the shape in current model is torch.Size([101, 64]).\n\tsize mismatch for transformer.net.layers.5.0.fn.proj_v: copying a param with shape torch.Size([101, 128]) from checkpoint, the shape in current model is torch.Size([101, 64]).\n\tsize mismatch for transformer.net.layers.5.0.fn.to_q.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for transformer.net.layers.5.0.fn.to_k.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for transformer.net.layers.5.0.fn.to_v.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for transformer.net.layers.5.0.fn.to_out.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for transformer.net.layers.5.0.fn.to_out.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for transformer.net.layers.5.0.norm.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for transformer.net.layers.5.0.norm.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for transformer.net.layers.5.1.fn.w1.weight: copying a param with shape torch.Size([1024, 256]) from checkpoint, the shape in current model is torch.Size([512, 128]).\n\tsize mismatch for transformer.net.layers.5.1.fn.w1.bias: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for transformer.net.layers.5.1.fn.w2.weight: copying a param with shape torch.Size([256, 1024]) from checkpoint, the shape in current model is torch.Size([128, 512]).\n\tsize mismatch for transformer.net.layers.5.1.fn.w2.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for transformer.net.layers.5.1.norm.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for transformer.net.layers.5.1.norm.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for transformer.net.layers.6.0.fn.proj_k: copying a param with shape torch.Size([101, 128]) from checkpoint, the shape in current model is torch.Size([101, 64]).\n\tsize mismatch for transformer.net.layers.6.0.fn.proj_v: copying a param with shape torch.Size([101, 128]) from checkpoint, the shape in current model is torch.Size([101, 64]).\n\tsize mismatch for transformer.net.layers.6.0.fn.to_q.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for transformer.net.layers.6.0.fn.to_k.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for transformer.net.layers.6.0.fn.to_v.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for transformer.net.layers.6.0.fn.to_out.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for transformer.net.layers.6.0.fn.to_out.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for transformer.net.layers.6.0.norm.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for transformer.net.layers.6.0.norm.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for transformer.net.layers.6.1.fn.w1.weight: copying a param with shape torch.Size([1024, 256]) from checkpoint, the shape in current model is torch.Size([512, 128]).\n\tsize mismatch for transformer.net.layers.6.1.fn.w1.bias: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for transformer.net.layers.6.1.fn.w2.weight: copying a param with shape torch.Size([256, 1024]) from checkpoint, the shape in current model is torch.Size([128, 512]).\n\tsize mismatch for transformer.net.layers.6.1.fn.w2.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for transformer.net.layers.6.1.norm.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for transformer.net.layers.6.1.norm.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for transformer.net.layers.7.0.fn.proj_k: copying a param with shape torch.Size([101, 128]) from checkpoint, the shape in current model is torch.Size([101, 64]).\n\tsize mismatch for transformer.net.layers.7.0.fn.proj_v: copying a param with shape torch.Size([101, 128]) from checkpoint, the shape in current model is torch.Size([101, 64]).\n\tsize mismatch for transformer.net.layers.7.0.fn.to_q.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for transformer.net.layers.7.0.fn.to_k.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for transformer.net.layers.7.0.fn.to_v.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for transformer.net.layers.7.0.fn.to_out.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for transformer.net.layers.7.0.fn.to_out.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for transformer.net.layers.7.0.norm.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for transformer.net.layers.7.0.norm.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for transformer.net.layers.7.1.fn.w1.weight: copying a param with shape torch.Size([1024, 256]) from checkpoint, the shape in current model is torch.Size([512, 128]).\n\tsize mismatch for transformer.net.layers.7.1.fn.w1.bias: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for transformer.net.layers.7.1.fn.w2.weight: copying a param with shape torch.Size([256, 1024]) from checkpoint, the shape in current model is torch.Size([128, 512]).\n\tsize mismatch for transformer.net.layers.7.1.fn.w2.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for transformer.net.layers.7.1.norm.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for transformer.net.layers.7.1.norm.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for transformer.net.layers.8.0.fn.proj_k: copying a param with shape torch.Size([101, 128]) from checkpoint, the shape in current model is torch.Size([101, 64]).\n\tsize mismatch for transformer.net.layers.8.0.fn.proj_v: copying a param with shape torch.Size([101, 128]) from checkpoint, the shape in current model is torch.Size([101, 64]).\n\tsize mismatch for transformer.net.layers.8.0.fn.to_q.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for transformer.net.layers.8.0.fn.to_k.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for transformer.net.layers.8.0.fn.to_v.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for transformer.net.layers.8.0.fn.to_out.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for transformer.net.layers.8.0.fn.to_out.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for transformer.net.layers.8.0.norm.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for transformer.net.layers.8.0.norm.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for transformer.net.layers.8.1.fn.w1.weight: copying a param with shape torch.Size([1024, 256]) from checkpoint, the shape in current model is torch.Size([512, 128]).\n\tsize mismatch for transformer.net.layers.8.1.fn.w1.bias: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for transformer.net.layers.8.1.fn.w2.weight: copying a param with shape torch.Size([256, 1024]) from checkpoint, the shape in current model is torch.Size([128, 512]).\n\tsize mismatch for transformer.net.layers.8.1.fn.w2.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for transformer.net.layers.8.1.norm.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for transformer.net.layers.8.1.norm.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for transformer.net.layers.9.0.fn.proj_k: copying a param with shape torch.Size([101, 128]) from checkpoint, the shape in current model is torch.Size([101, 64]).\n\tsize mismatch for transformer.net.layers.9.0.fn.proj_v: copying a param with shape torch.Size([101, 128]) from checkpoint, the shape in current model is torch.Size([101, 64]).\n\tsize mismatch for transformer.net.layers.9.0.fn.to_q.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for transformer.net.layers.9.0.fn.to_k.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for transformer.net.layers.9.0.fn.to_v.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for transformer.net.layers.9.0.fn.to_out.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for transformer.net.layers.9.0.fn.to_out.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for transformer.net.layers.9.0.norm.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for transformer.net.layers.9.0.norm.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for transformer.net.layers.9.1.fn.w1.weight: copying a param with shape torch.Size([1024, 256]) from checkpoint, the shape in current model is torch.Size([512, 128]).\n\tsize mismatch for transformer.net.layers.9.1.fn.w1.bias: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for transformer.net.layers.9.1.fn.w2.weight: copying a param with shape torch.Size([256, 1024]) from checkpoint, the shape in current model is torch.Size([128, 512]).\n\tsize mismatch for transformer.net.layers.9.1.fn.w2.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for transformer.net.layers.9.1.norm.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for transformer.net.layers.9.1.norm.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for transformer.net.layers.10.0.fn.proj_k: copying a param with shape torch.Size([101, 128]) from checkpoint, the shape in current model is torch.Size([101, 64]).\n\tsize mismatch for transformer.net.layers.10.0.fn.proj_v: copying a param with shape torch.Size([101, 128]) from checkpoint, the shape in current model is torch.Size([101, 64]).\n\tsize mismatch for transformer.net.layers.10.0.fn.to_q.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for transformer.net.layers.10.0.fn.to_k.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for transformer.net.layers.10.0.fn.to_v.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for transformer.net.layers.10.0.fn.to_out.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for transformer.net.layers.10.0.fn.to_out.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for transformer.net.layers.10.0.norm.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for transformer.net.layers.10.0.norm.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for transformer.net.layers.10.1.fn.w1.weight: copying a param with shape torch.Size([1024, 256]) from checkpoint, the shape in current model is torch.Size([512, 128]).\n\tsize mismatch for transformer.net.layers.10.1.fn.w1.bias: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for transformer.net.layers.10.1.fn.w2.weight: copying a param with shape torch.Size([256, 1024]) from checkpoint, the shape in current model is torch.Size([128, 512]).\n\tsize mismatch for transformer.net.layers.10.1.fn.w2.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for transformer.net.layers.10.1.norm.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for transformer.net.layers.10.1.norm.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for transformer.net.layers.11.0.fn.proj_k: copying a param with shape torch.Size([101, 128]) from checkpoint, the shape in current model is torch.Size([101, 64]).\n\tsize mismatch for transformer.net.layers.11.0.fn.proj_v: copying a param with shape torch.Size([101, 128]) from checkpoint, the shape in current model is torch.Size([101, 64]).\n\tsize mismatch for transformer.net.layers.11.0.fn.to_q.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for transformer.net.layers.11.0.fn.to_k.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for transformer.net.layers.11.0.fn.to_v.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for transformer.net.layers.11.0.fn.to_out.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for transformer.net.layers.11.0.fn.to_out.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for transformer.net.layers.11.0.norm.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for transformer.net.layers.11.0.norm.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for transformer.net.layers.11.1.fn.w1.weight: copying a param with shape torch.Size([1024, 256]) from checkpoint, the shape in current model is torch.Size([512, 128]).\n\tsize mismatch for transformer.net.layers.11.1.fn.w1.bias: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for transformer.net.layers.11.1.fn.w2.weight: copying a param with shape torch.Size([256, 1024]) from checkpoint, the shape in current model is torch.Size([128, 512]).\n\tsize mismatch for transformer.net.layers.11.1.fn.w2.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for transformer.net.layers.11.1.norm.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for transformer.net.layers.11.1.norm.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for mlp_head.0.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for mlp_head.0.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for mlp_head.1.weight: copying a param with shape torch.Size([2, 256]) from checkpoint, the shape in current model is torch.Size([2, 128]).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 84\u001b[0m\n\u001b[1;32m     82\u001b[0m efficient_transformer \u001b[38;5;241m=\u001b[39m Linformer(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m128\u001b[39m, seq_len\u001b[38;5;241m=\u001b[39m(IMG_SIZE \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m patch_size) \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m, depth\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m12\u001b[39m, heads\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m, k\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m64\u001b[39m)\n\u001b[1;32m     83\u001b[0m model \u001b[38;5;241m=\u001b[39m ViT(image_size\u001b[38;5;241m=\u001b[39mIMG_SIZE, patch_size\u001b[38;5;241m=\u001b[39mpatch_size, num_classes\u001b[38;5;241m=\u001b[39mnum_classes, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m128\u001b[39m, transformer\u001b[38;5;241m=\u001b[39mefficient_transformer, channels\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m---> 84\u001b[0m model\u001b[38;5;241m.\u001b[39mload_state_dict(torch\u001b[38;5;241m.\u001b[39mload(PATH))\n\u001b[1;32m     86\u001b[0m \u001b[38;5;66;03m# Function to calculate overall accuracy\u001b[39;00m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21moverall_accuracy\u001b[39m(model, test_loader, criterion):\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:2189\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[0;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[1;32m   2184\u001b[0m         error_msgs\u001b[38;5;241m.\u001b[39minsert(\n\u001b[1;32m   2185\u001b[0m             \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2186\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)))\n\u001b[1;32m   2188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 2189\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2190\u001b[0m                        \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)))\n\u001b[1;32m   2191\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for ViT:\n\tUnexpected key(s) in state_dict: \"transformer.net.layers.12.0.fn.proj_k\", \"transformer.net.layers.12.0.fn.proj_v\", \"transformer.net.layers.12.0.fn.to_q.weight\", \"transformer.net.layers.12.0.fn.to_k.weight\", \"transformer.net.layers.12.0.fn.to_v.weight\", \"transformer.net.layers.12.0.fn.to_out.weight\", \"transformer.net.layers.12.0.fn.to_out.bias\", \"transformer.net.layers.12.0.norm.weight\", \"transformer.net.layers.12.0.norm.bias\", \"transformer.net.layers.12.1.fn.w1.weight\", \"transformer.net.layers.12.1.fn.w1.bias\", \"transformer.net.layers.12.1.fn.w2.weight\", \"transformer.net.layers.12.1.fn.w2.bias\", \"transformer.net.layers.12.1.norm.weight\", \"transformer.net.layers.12.1.norm.bias\", \"transformer.net.layers.13.0.fn.proj_k\", \"transformer.net.layers.13.0.fn.proj_v\", \"transformer.net.layers.13.0.fn.to_q.weight\", \"transformer.net.layers.13.0.fn.to_k.weight\", \"transformer.net.layers.13.0.fn.to_v.weight\", \"transformer.net.layers.13.0.fn.to_out.weight\", \"transformer.net.layers.13.0.fn.to_out.bias\", \"transformer.net.layers.13.0.norm.weight\", \"transformer.net.layers.13.0.norm.bias\", \"transformer.net.layers.13.1.fn.w1.weight\", \"transformer.net.layers.13.1.fn.w1.bias\", \"transformer.net.layers.13.1.fn.w2.weight\", \"transformer.net.layers.13.1.fn.w2.bias\", \"transformer.net.layers.13.1.norm.weight\", \"transformer.net.layers.13.1.norm.bias\", \"transformer.net.layers.14.0.fn.proj_k\", \"transformer.net.layers.14.0.fn.proj_v\", \"transformer.net.layers.14.0.fn.to_q.weight\", \"transformer.net.layers.14.0.fn.to_k.weight\", \"transformer.net.layers.14.0.fn.to_v.weight\", \"transformer.net.layers.14.0.fn.to_out.weight\", \"transformer.net.layers.14.0.fn.to_out.bias\", \"transformer.net.layers.14.0.norm.weight\", \"transformer.net.layers.14.0.norm.bias\", \"transformer.net.layers.14.1.fn.w1.weight\", \"transformer.net.layers.14.1.fn.w1.bias\", \"transformer.net.layers.14.1.fn.w2.weight\", \"transformer.net.layers.14.1.fn.w2.bias\", \"transformer.net.layers.14.1.norm.weight\", \"transformer.net.layers.14.1.norm.bias\", \"transformer.net.layers.15.0.fn.proj_k\", \"transformer.net.layers.15.0.fn.proj_v\", \"transformer.net.layers.15.0.fn.to_q.weight\", \"transformer.net.layers.15.0.fn.to_k.weight\", \"transformer.net.layers.15.0.fn.to_v.weight\", \"transformer.net.layers.15.0.fn.to_out.weight\", \"transformer.net.layers.15.0.fn.to_out.bias\", \"transformer.net.layers.15.0.norm.weight\", \"transformer.net.layers.15.0.norm.bias\", \"transformer.net.layers.15.1.fn.w1.weight\", \"transformer.net.layers.15.1.fn.w1.bias\", \"transformer.net.layers.15.1.fn.w2.weight\", \"transformer.net.layers.15.1.fn.w2.bias\", \"transformer.net.layers.15.1.norm.weight\", \"transformer.net.layers.15.1.norm.bias\", \"transformer.net.layers.16.0.fn.proj_k\", \"transformer.net.layers.16.0.fn.proj_v\", \"transformer.net.layers.16.0.fn.to_q.weight\", \"transformer.net.layers.16.0.fn.to_k.weight\", \"transformer.net.layers.16.0.fn.to_v.weight\", \"transformer.net.layers.16.0.fn.to_out.weight\", \"transformer.net.layers.16.0.fn.to_out.bias\", \"transformer.net.layers.16.0.norm.weight\", \"transformer.net.layers.16.0.norm.bias\", \"transformer.net.layers.16.1.fn.w1.weight\", \"transformer.net.layers.16.1.fn.w1.bias\", \"transformer.net.layers.16.1.fn.w2.weight\", \"transformer.net.layers.16.1.fn.w2.bias\", \"transformer.net.layers.16.1.norm.weight\", \"transformer.net.layers.16.1.norm.bias\", \"transformer.net.layers.17.0.fn.proj_k\", \"transformer.net.layers.17.0.fn.proj_v\", \"transformer.net.layers.17.0.fn.to_q.weight\", \"transformer.net.layers.17.0.fn.to_k.weight\", \"transformer.net.layers.17.0.fn.to_v.weight\", \"transformer.net.layers.17.0.fn.to_out.weight\", \"transformer.net.layers.17.0.fn.to_out.bias\", \"transformer.net.layers.17.0.norm.weight\", \"transformer.net.layers.17.0.norm.bias\", \"transformer.net.layers.17.1.fn.w1.weight\", \"transformer.net.layers.17.1.fn.w1.bias\", \"transformer.net.layers.17.1.fn.w2.weight\", \"transformer.net.layers.17.1.fn.w2.bias\", \"transformer.net.layers.17.1.norm.weight\", \"transformer.net.layers.17.1.norm.bias\", \"transformer.net.layers.18.0.fn.proj_k\", \"transformer.net.layers.18.0.fn.proj_v\", \"transformer.net.layers.18.0.fn.to_q.weight\", \"transformer.net.layers.18.0.fn.to_k.weight\", \"transformer.net.layers.18.0.fn.to_v.weight\", \"transformer.net.layers.18.0.fn.to_out.weight\", \"transformer.net.layers.18.0.fn.to_out.bias\", \"transformer.net.layers.18.0.norm.weight\", \"transformer.net.layers.18.0.norm.bias\", \"transformer.net.layers.18.1.fn.w1.weight\", \"transformer.net.layers.18.1.fn.w1.bias\", \"transformer.net.layers.18.1.fn.w2.weight\", \"transformer.net.layers.18.1.fn.w2.bias\", \"transformer.net.layers.18.1.norm.weight\", \"transformer.net.layers.18.1.norm.bias\", \"transformer.net.layers.19.0.fn.proj_k\", \"transformer.net.layers.19.0.fn.proj_v\", \"transformer.net.layers.19.0.fn.to_q.weight\", \"transformer.net.layers.19.0.fn.to_k.weight\", \"transformer.net.layers.19.0.fn.to_v.weight\", \"transformer.net.layers.19.0.fn.to_out.weight\", \"transformer.net.layers.19.0.fn.to_out.bias\", \"transformer.net.layers.19.0.norm.weight\", \"transformer.net.layers.19.0.norm.bias\", \"transformer.net.layers.19.1.fn.w1.weight\", \"transformer.net.layers.19.1.fn.w1.bias\", \"transformer.net.layers.19.1.fn.w2.weight\", \"transformer.net.layers.19.1.fn.w2.bias\", \"transformer.net.layers.19.1.norm.weight\", \"transformer.net.layers.19.1.norm.bias\", \"transformer.net.layers.20.0.fn.proj_k\", \"transformer.net.layers.20.0.fn.proj_v\", \"transformer.net.layers.20.0.fn.to_q.weight\", \"transformer.net.layers.20.0.fn.to_k.weight\", \"transformer.net.layers.20.0.fn.to_v.weight\", \"transformer.net.layers.20.0.fn.to_out.weight\", \"transformer.net.layers.20.0.fn.to_out.bias\", \"transformer.net.layers.20.0.norm.weight\", \"transformer.net.layers.20.0.norm.bias\", \"transformer.net.layers.20.1.fn.w1.weight\", \"transformer.net.layers.20.1.fn.w1.bias\", \"transformer.net.layers.20.1.fn.w2.weight\", \"transformer.net.layers.20.1.fn.w2.bias\", \"transformer.net.layers.20.1.norm.weight\", \"transformer.net.layers.20.1.norm.bias\", \"transformer.net.layers.21.0.fn.proj_k\", \"transformer.net.layers.21.0.fn.proj_v\", \"transformer.net.layers.21.0.fn.to_q.weight\", \"transformer.net.layers.21.0.fn.to_k.weight\", \"transformer.net.layers.21.0.fn.to_v.weight\", \"transformer.net.layers.21.0.fn.to_out.weight\", \"transformer.net.layers.21.0.fn.to_out.bias\", \"transformer.net.layers.21.0.norm.weight\", \"transformer.net.layers.21.0.norm.bias\", \"transformer.net.layers.21.1.fn.w1.weight\", \"transformer.net.layers.21.1.fn.w1.bias\", \"transformer.net.layers.21.1.fn.w2.weight\", \"transformer.net.layers.21.1.fn.w2.bias\", \"transformer.net.layers.21.1.norm.weight\", \"transformer.net.layers.21.1.norm.bias\", \"transformer.net.layers.22.0.fn.proj_k\", \"transformer.net.layers.22.0.fn.proj_v\", \"transformer.net.layers.22.0.fn.to_q.weight\", \"transformer.net.layers.22.0.fn.to_k.weight\", \"transformer.net.layers.22.0.fn.to_v.weight\", \"transformer.net.layers.22.0.fn.to_out.weight\", \"transformer.net.layers.22.0.fn.to_out.bias\", \"transformer.net.layers.22.0.norm.weight\", \"transformer.net.layers.22.0.norm.bias\", \"transformer.net.layers.22.1.fn.w1.weight\", \"transformer.net.layers.22.1.fn.w1.bias\", \"transformer.net.layers.22.1.fn.w2.weight\", \"transformer.net.layers.22.1.fn.w2.bias\", \"transformer.net.layers.22.1.norm.weight\", \"transformer.net.layers.22.1.norm.bias\", \"transformer.net.layers.23.0.fn.proj_k\", \"transformer.net.layers.23.0.fn.proj_v\", \"transformer.net.layers.23.0.fn.to_q.weight\", \"transformer.net.layers.23.0.fn.to_k.weight\", \"transformer.net.layers.23.0.fn.to_v.weight\", \"transformer.net.layers.23.0.fn.to_out.weight\", \"transformer.net.layers.23.0.fn.to_out.bias\", \"transformer.net.layers.23.0.norm.weight\", \"transformer.net.layers.23.0.norm.bias\", \"transformer.net.layers.23.1.fn.w1.weight\", \"transformer.net.layers.23.1.fn.w1.bias\", \"transformer.net.layers.23.1.fn.w2.weight\", \"transformer.net.layers.23.1.fn.w2.bias\", \"transformer.net.layers.23.1.norm.weight\", \"transformer.net.layers.23.1.norm.bias\". \n\tsize mismatch for pos_embedding: copying a param with shape torch.Size([1, 101, 256]) from checkpoint, the shape in current model is torch.Size([1, 101, 128]).\n\tsize mismatch for cls_token: copying a param with shape torch.Size([1, 1, 256]) from checkpoint, the shape in current model is torch.Size([1, 1, 128]).\n\tsize mismatch for to_patch_embedding.2.weight: copying a param with shape torch.Size([256, 1200]) from checkpoint, the shape in current model is torch.Size([128, 1200]).\n\tsize mismatch for to_patch_embedding.2.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for to_patch_embedding.3.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for to_patch_embedding.3.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for transformer.net.layers.0.0.fn.proj_k: copying a param with shape torch.Size([101, 128]) from checkpoint, the shape in current model is torch.Size([101, 64]).\n\tsize mismatch for transformer.net.layers.0.0.fn.proj_v: copying a param with shape torch.Size([101, 128]) from checkpoint, the shape in current model is torch.Size([101, 64]).\n\tsize mismatch for transformer.net.layers.0.0.fn.to_q.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for transformer.net.layers.0.0.fn.to_k.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for transformer.net.layers.0.0.fn.to_v.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for transformer.net.layers.0.0.fn.to_out.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for transformer.net.layers.0.0.fn.to_out.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for transformer.net.layers.0.0.norm.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for transformer.net.layers.0.0.norm.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for transformer.net.layers.0.1.fn.w1.weight: copying a param with shape torch.Size([1024, 256]) from checkpoint, the shape in current model is torch.Size([512, 128]).\n\tsize mismatch for transformer.net.layers.0.1.fn.w1.bias: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for transformer.net.layers.0.1.fn.w2.weight: copying a param with shape torch.Size([256, 1024]) from checkpoint, the shape in current model is torch.Size([128, 512]).\n\tsize mismatch for transformer.net.layers.0.1.fn.w2.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for transformer.net.layers.0.1.norm.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for transformer.net.layers.0.1.norm.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for transformer.net.layers.1.0.fn.proj_k: copying a param with shape torch.Size([101, 128]) from checkpoint, the shape in current model is torch.Size([101, 64]).\n\tsize mismatch for transformer.net.layers.1.0.fn.proj_v: copying a param with shape torch.Size([101, 128]) from checkpoint, the shape in current model is torch.Size([101, 64]).\n\tsize mismatch for transformer.net.layers.1.0.fn.to_q.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for transformer.net.layers.1.0.fn.to_k.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for transformer.net.layers.1.0.fn.to_v.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for transformer.net.layers.1.0.fn.to_out.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for transformer.net.layers.1.0.fn.to_out.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for transformer.net.layers.1.0.norm.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for transformer.net.layers.1.0.norm.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for transformer.net.layers.1.1.fn.w1.weight: copying a param with shape torch.Size([1024, 256]) from checkpoint, the shape in current model is torch.Size([512, 128]).\n\tsize mismatch for transformer.net.layers.1.1.fn.w1.bias: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for transformer.net.layers.1.1.fn.w2.weight: copying a param with shape torch.Size([256, 1024]) from checkpoint, the shape in current model is torch.Size([128, 512]).\n\tsize mismatch for transformer.net.layers.1.1.fn.w2.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for transformer.net.layers.1.1.norm.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for transformer.net.layers.1.1.norm.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for transformer.net.layers.2.0.fn.proj_k: copying a param with shape torch.Size([101, 128]) from checkpoint, the shape in current model is torch.Size([101, 64]).\n\tsize mismatch for transformer.net.layers.2.0.fn.proj_v: copying a param with shape torch.Size([101, 128]) from checkpoint, the shape in current model is torch.Size([101, 64]).\n\tsize mismatch for transformer.net.layers.2.0.fn.to_q.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for transformer.net.layers.2.0.fn.to_k.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for transformer.net.layers.2.0.fn.to_v.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for transformer.net.layers.2.0.fn.to_out.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for transformer.net.layers.2.0.fn.to_out.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for transformer.net.layers.2.0.norm.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for transformer.net.layers.2.0.norm.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for transformer.net.layers.2.1.fn.w1.weight: copying a param with shape torch.Size([1024, 256]) from checkpoint, the shape in current model is torch.Size([512, 128]).\n\tsize mismatch for transformer.net.layers.2.1.fn.w1.bias: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for transformer.net.layers.2.1.fn.w2.weight: copying a param with shape torch.Size([256, 1024]) from checkpoint, the shape in current model is torch.Size([128, 512]).\n\tsize mismatch for transformer.net.layers.2.1.fn.w2.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for transformer.net.layers.2.1.norm.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for transformer.net.layers.2.1.norm.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for transformer.net.layers.3.0.fn.proj_k: copying a param with shape torch.Size([101, 128]) from checkpoint, the shape in current model is torch.Size([101, 64]).\n\tsize mismatch for transformer.net.layers.3.0.fn.proj_v: copying a param with shape torch.Size([101, 128]) from checkpoint, the shape in current model is torch.Size([101, 64]).\n\tsize mismatch for transformer.net.layers.3.0.fn.to_q.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for transformer.net.layers.3.0.fn.to_k.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for transformer.net.layers.3.0.fn.to_v.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for transformer.net.layers.3.0.fn.to_out.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for transformer.net.layers.3.0.fn.to_out.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for transformer.net.layers.3.0.norm.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for transformer.net.layers.3.0.norm.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for transformer.net.layers.3.1.fn.w1.weight: copying a param with shape torch.Size([1024, 256]) from checkpoint, the shape in current model is torch.Size([512, 128]).\n\tsize mismatch for transformer.net.layers.3.1.fn.w1.bias: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for transformer.net.layers.3.1.fn.w2.weight: copying a param with shape torch.Size([256, 1024]) from checkpoint, the shape in current model is torch.Size([128, 512]).\n\tsize mismatch for transformer.net.layers.3.1.fn.w2.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for transformer.net.layers.3.1.norm.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for transformer.net.layers.3.1.norm.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for transformer.net.layers.4.0.fn.proj_k: copying a param with shape torch.Size([101, 128]) from checkpoint, the shape in current model is torch.Size([101, 64]).\n\tsize mismatch for transformer.net.layers.4.0.fn.proj_v: copying a param with shape torch.Size([101, 128]) from checkpoint, the shape in current model is torch.Size([101, 64]).\n\tsize mismatch for transformer.net.layers.4.0.fn.to_q.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for transformer.net.layers.4.0.fn.to_k.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for transformer.net.layers.4.0.fn.to_v.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for transformer.net.layers.4.0.fn.to_out.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for transformer.net.layers.4.0.fn.to_out.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for transformer.net.layers.4.0.norm.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for transformer.net.layers.4.0.norm.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for transformer.net.layers.4.1.fn.w1.weight: copying a param with shape torch.Size([1024, 256]) from checkpoint, the shape in current model is torch.Size([512, 128]).\n\tsize mismatch for transformer.net.layers.4.1.fn.w1.bias: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for transformer.net.layers.4.1.fn.w2.weight: copying a param with shape torch.Size([256, 1024]) from checkpoint, the shape in current model is torch.Size([128, 512]).\n\tsize mismatch for transformer.net.layers.4.1.fn.w2.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for transformer.net.layers.4.1.norm.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for transformer.net.layers.4.1.norm.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for transformer.net.layers.5.0.fn.proj_k: copying a param with shape torch.Size([101, 128]) from checkpoint, the shape in current model is torch.Size([101, 64]).\n\tsize mismatch for transformer.net.layers.5.0.fn.proj_v: copying a param with shape torch.Size([101, 128]) from checkpoint, the shape in current model is torch.Size([101, 64]).\n\tsize mismatch for transformer.net.layers.5.0.fn.to_q.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for transformer.net.layers.5.0.fn.to_k.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for transformer.net.layers.5.0.fn.to_v.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for transformer.net.layers.5.0.fn.to_out.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for transformer.net.layers.5.0.fn.to_out.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for transformer.net.layers.5.0.norm.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for transformer.net.layers.5.0.norm.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for transformer.net.layers.5.1.fn.w1.weight: copying a param with shape torch.Size([1024, 256]) from checkpoint, the shape in current model is torch.Size([512, 128]).\n\tsize mismatch for transformer.net.layers.5.1.fn.w1.bias: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for transformer.net.layers.5.1.fn.w2.weight: copying a param with shape torch.Size([256, 1024]) from checkpoint, the shape in current model is torch.Size([128, 512]).\n\tsize mismatch for transformer.net.layers.5.1.fn.w2.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for transformer.net.layers.5.1.norm.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for transformer.net.layers.5.1.norm.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for transformer.net.layers.6.0.fn.proj_k: copying a param with shape torch.Size([101, 128]) from checkpoint, the shape in current model is torch.Size([101, 64]).\n\tsize mismatch for transformer.net.layers.6.0.fn.proj_v: copying a param with shape torch.Size([101, 128]) from checkpoint, the shape in current model is torch.Size([101, 64]).\n\tsize mismatch for transformer.net.layers.6.0.fn.to_q.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for transformer.net.layers.6.0.fn.to_k.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for transformer.net.layers.6.0.fn.to_v.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for transformer.net.layers.6.0.fn.to_out.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for transformer.net.layers.6.0.fn.to_out.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for transformer.net.layers.6.0.norm.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for transformer.net.layers.6.0.norm.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for transformer.net.layers.6.1.fn.w1.weight: copying a param with shape torch.Size([1024, 256]) from checkpoint, the shape in current model is torch.Size([512, 128]).\n\tsize mismatch for transformer.net.layers.6.1.fn.w1.bias: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for transformer.net.layers.6.1.fn.w2.weight: copying a param with shape torch.Size([256, 1024]) from checkpoint, the shape in current model is torch.Size([128, 512]).\n\tsize mismatch for transformer.net.layers.6.1.fn.w2.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for transformer.net.layers.6.1.norm.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for transformer.net.layers.6.1.norm.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for transformer.net.layers.7.0.fn.proj_k: copying a param with shape torch.Size([101, 128]) from checkpoint, the shape in current model is torch.Size([101, 64]).\n\tsize mismatch for transformer.net.layers.7.0.fn.proj_v: copying a param with shape torch.Size([101, 128]) from checkpoint, the shape in current model is torch.Size([101, 64]).\n\tsize mismatch for transformer.net.layers.7.0.fn.to_q.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for transformer.net.layers.7.0.fn.to_k.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for transformer.net.layers.7.0.fn.to_v.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for transformer.net.layers.7.0.fn.to_out.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for transformer.net.layers.7.0.fn.to_out.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for transformer.net.layers.7.0.norm.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for transformer.net.layers.7.0.norm.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for transformer.net.layers.7.1.fn.w1.weight: copying a param with shape torch.Size([1024, 256]) from checkpoint, the shape in current model is torch.Size([512, 128]).\n\tsize mismatch for transformer.net.layers.7.1.fn.w1.bias: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for transformer.net.layers.7.1.fn.w2.weight: copying a param with shape torch.Size([256, 1024]) from checkpoint, the shape in current model is torch.Size([128, 512]).\n\tsize mismatch for transformer.net.layers.7.1.fn.w2.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for transformer.net.layers.7.1.norm.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for transformer.net.layers.7.1.norm.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for transformer.net.layers.8.0.fn.proj_k: copying a param with shape torch.Size([101, 128]) from checkpoint, the shape in current model is torch.Size([101, 64]).\n\tsize mismatch for transformer.net.layers.8.0.fn.proj_v: copying a param with shape torch.Size([101, 128]) from checkpoint, the shape in current model is torch.Size([101, 64]).\n\tsize mismatch for transformer.net.layers.8.0.fn.to_q.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for transformer.net.layers.8.0.fn.to_k.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for transformer.net.layers.8.0.fn.to_v.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for transformer.net.layers.8.0.fn.to_out.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for transformer.net.layers.8.0.fn.to_out.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for transformer.net.layers.8.0.norm.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for transformer.net.layers.8.0.norm.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for transformer.net.layers.8.1.fn.w1.weight: copying a param with shape torch.Size([1024, 256]) from checkpoint, the shape in current model is torch.Size([512, 128]).\n\tsize mismatch for transformer.net.layers.8.1.fn.w1.bias: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for transformer.net.layers.8.1.fn.w2.weight: copying a param with shape torch.Size([256, 1024]) from checkpoint, the shape in current model is torch.Size([128, 512]).\n\tsize mismatch for transformer.net.layers.8.1.fn.w2.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for transformer.net.layers.8.1.norm.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for transformer.net.layers.8.1.norm.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for transformer.net.layers.9.0.fn.proj_k: copying a param with shape torch.Size([101, 128]) from checkpoint, the shape in current model is torch.Size([101, 64]).\n\tsize mismatch for transformer.net.layers.9.0.fn.proj_v: copying a param with shape torch.Size([101, 128]) from checkpoint, the shape in current model is torch.Size([101, 64]).\n\tsize mismatch for transformer.net.layers.9.0.fn.to_q.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for transformer.net.layers.9.0.fn.to_k.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for transformer.net.layers.9.0.fn.to_v.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for transformer.net.layers.9.0.fn.to_out.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for transformer.net.layers.9.0.fn.to_out.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for transformer.net.layers.9.0.norm.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for transformer.net.layers.9.0.norm.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for transformer.net.layers.9.1.fn.w1.weight: copying a param with shape torch.Size([1024, 256]) from checkpoint, the shape in current model is torch.Size([512, 128]).\n\tsize mismatch for transformer.net.layers.9.1.fn.w1.bias: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for transformer.net.layers.9.1.fn.w2.weight: copying a param with shape torch.Size([256, 1024]) from checkpoint, the shape in current model is torch.Size([128, 512]).\n\tsize mismatch for transformer.net.layers.9.1.fn.w2.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for transformer.net.layers.9.1.norm.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for transformer.net.layers.9.1.norm.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for transformer.net.layers.10.0.fn.proj_k: copying a param with shape torch.Size([101, 128]) from checkpoint, the shape in current model is torch.Size([101, 64]).\n\tsize mismatch for transformer.net.layers.10.0.fn.proj_v: copying a param with shape torch.Size([101, 128]) from checkpoint, the shape in current model is torch.Size([101, 64]).\n\tsize mismatch for transformer.net.layers.10.0.fn.to_q.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for transformer.net.layers.10.0.fn.to_k.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for transformer.net.layers.10.0.fn.to_v.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for transformer.net.layers.10.0.fn.to_out.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for transformer.net.layers.10.0.fn.to_out.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for transformer.net.layers.10.0.norm.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for transformer.net.layers.10.0.norm.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for transformer.net.layers.10.1.fn.w1.weight: copying a param with shape torch.Size([1024, 256]) from checkpoint, the shape in current model is torch.Size([512, 128]).\n\tsize mismatch for transformer.net.layers.10.1.fn.w1.bias: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for transformer.net.layers.10.1.fn.w2.weight: copying a param with shape torch.Size([256, 1024]) from checkpoint, the shape in current model is torch.Size([128, 512]).\n\tsize mismatch for transformer.net.layers.10.1.fn.w2.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for transformer.net.layers.10.1.norm.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for transformer.net.layers.10.1.norm.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for transformer.net.layers.11.0.fn.proj_k: copying a param with shape torch.Size([101, 128]) from checkpoint, the shape in current model is torch.Size([101, 64]).\n\tsize mismatch for transformer.net.layers.11.0.fn.proj_v: copying a param with shape torch.Size([101, 128]) from checkpoint, the shape in current model is torch.Size([101, 64]).\n\tsize mismatch for transformer.net.layers.11.0.fn.to_q.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for transformer.net.layers.11.0.fn.to_k.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for transformer.net.layers.11.0.fn.to_v.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for transformer.net.layers.11.0.fn.to_out.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([128, 128]).\n\tsize mismatch for transformer.net.layers.11.0.fn.to_out.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for transformer.net.layers.11.0.norm.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for transformer.net.layers.11.0.norm.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for transformer.net.layers.11.1.fn.w1.weight: copying a param with shape torch.Size([1024, 256]) from checkpoint, the shape in current model is torch.Size([512, 128]).\n\tsize mismatch for transformer.net.layers.11.1.fn.w1.bias: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([512]).\n\tsize mismatch for transformer.net.layers.11.1.fn.w2.weight: copying a param with shape torch.Size([256, 1024]) from checkpoint, the shape in current model is torch.Size([128, 512]).\n\tsize mismatch for transformer.net.layers.11.1.fn.w2.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for transformer.net.layers.11.1.norm.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for transformer.net.layers.11.1.norm.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for mlp_head.0.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for mlp_head.0.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for mlp_head.1.weight: copying a param with shape torch.Size([2, 256]) from checkpoint, the shape in current model is torch.Size([2, 128])."
     ]
    }
   ],
   "source": [
    "# Define paths\n",
    "# train_dir = '/content/drive/MyDrive/dataset/training'\n",
    "# val_dir = '/content/drive/MyDrive/dataset/test'\n",
    "# test_dir = '/content/drive/MyDrive/dataset/validation'\n",
    "\n",
    "\n",
    "train_dir = 'training_files/target_training_datasets/CHEMBL286/dataset/training'\n",
    "val_dir = 'training_files/target_training_datasets/CHEMBL286/dataset/validation'\n",
    "test_dir = 'training_files/target_training_datasets/CHEMBL286/dataset/test'\n",
    "\n",
    "\n",
    "# Load datasets\n",
    "train_ds = torchvision.datasets.ImageFolder(train_dir, transform=transform)\n",
    "valid_ds = torchvision.datasets.ImageFolder(val_dir, transform=transform)\n",
    "test_ds = torchvision.datasets.ImageFolder(test_dir, transform=transform)\n",
    "\n",
    "# Data loaders\n",
    "train_loader = data.DataLoader(train_ds, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "valid_loader = data.DataLoader(valid_ds, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "test_loader = data.DataLoader(test_ds, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "\n",
    "# Linear Transformer\n",
    "efficient_transformer = Linformer(dim=256, seq_len=(IMG_SIZE // patch_size) ** 2 + 1, depth=24, heads=16, k=128)\n",
    "\n",
    "# Vision Transformer Model\n",
    "model = ViT(\n",
    "    dim=256,\n",
    "    image_size=IMG_SIZE,\n",
    "    patch_size=patch_size,\n",
    "    num_classes=num_classes,\n",
    "    transformer=efficient_transformer,\n",
    "    channels=3,\n",
    ").to(device)\n",
    "\n",
    "# Loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=1e-5)\n",
    "\n",
    "# Learning rate scheduler\n",
    "scheduler = StepLR(optimizer, step_size=10, gamma=gamma)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    epoch_accuracy = 0\n",
    "    for data, label in tqdm(train_loader):\n",
    "        data, label = data.to(device), label.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        acc = (output.argmax(dim=1) == label).float().mean()\n",
    "        epoch_accuracy += acc / len(train_loader)\n",
    "        epoch_loss += loss / len(train_loader)\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        epoch_val_accuracy = 0\n",
    "        epoch_val_loss = 0\n",
    "        for data, label in valid_loader:\n",
    "            data, label = data.to(device), label.to(device)\n",
    "            val_output = model(data)\n",
    "            val_loss = criterion(val_output, label)\n",
    "\n",
    "            acc = (val_output.argmax(dim=1) == label).float().mean()\n",
    "            epoch_val_accuracy += acc / len(valid_loader)\n",
    "            epoch_val_loss += val_loss / len(valid_loader)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {epoch_loss:.4f}, Accuracy: {epoch_accuracy:.4f}, Val Loss: {epoch_val_loss:.4f}, Val Accuracy: {epoch_val_accuracy:.4f}\")\n",
    "\n",
    "# Save the model\n",
    "PATH = f\"epochs_{epochs}img{IMG_SIZE}patch{patch_size}lr{lr}.pt\"\n",
    "torch.save(model.state_dict(), PATH)\n",
    "\n",
    "# Load saved model\n",
    "efficient_transformer = Linformer(dim=128, seq_len=(IMG_SIZE // patch_size) ** 2 + 1, depth=12, heads=8, k=64)\n",
    "model = ViT(image_size=IMG_SIZE, patch_size=patch_size, num_classes=num_classes, dim=128, transformer=efficient_transformer, channels=3).to(device)\n",
    "model.load_state_dict(torch.load(PATH))\n",
    "\n",
    "# Function to calculate overall accuracy\n",
    "def overall_accuracy(model, test_loader, criterion):\n",
    "    model.eval()\n",
    "    y_proba = []\n",
    "    y_truth = []\n",
    "    test_loss = 0\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, label in tqdm(test_loader):\n",
    "            data, label = data.to(device), label.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += criterion(output, label.long()).item()\n",
    "            for index, i in enumerate(output):\n",
    "                y_proba.append(i[1].item())\n",
    "                y_truth.append(label[index].item())\n",
    "                if torch.argmax(i) == label[index]:\n",
    "                    correct += 1\n",
    "                total += 1\n",
    "    accuracy = correct / total\n",
    "    y_proba_out = np.array(y_proba)\n",
    "    y_truth_out = np.array(y_truth)\n",
    "    return test_loss, accuracy, y_proba_out, y_truth_out\n",
    "\n",
    "# Evaluate model on test data\n",
    "loss, acc, y_proba, y_truth = overall_accuracy(model, test_loader, criterion)\n",
    "\n",
    "print(f\"Test Accuracy: {acc:.4f}\")\n",
    "\n",
    "# Plot confusion matrix\n",
    "cm = confusion_matrix(y_truth, np.argmax(y_proba.reshape(-1, 1), axis=1))\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "376a57aa3ef445d1a123d81fd90e5a43",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.4005\n"
     ]
    }
   ],
   "source": [
    "# Function to calculate overall accuracy\n",
    "def overall_accuracy(model, test_loader, criterion):\n",
    "    model.eval()\n",
    "    y_proba = []\n",
    "    y_truth = []\n",
    "    test_loss = 0\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, label in tqdm(test_loader):\n",
    "            data, label = data.to(device), label.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += criterion(output, label.long()).item()\n",
    "            for index, i in enumerate(output):\n",
    "                y_proba.append(i[1].item())\n",
    "                y_truth.append(label[index].item())\n",
    "                if torch.argmax(i) == label[index]:\n",
    "                    correct += 1\n",
    "                total += 1\n",
    "    accuracy = correct / total\n",
    "    y_proba_out = np.array(y_proba)\n",
    "    y_truth_out = np.array(y_truth)\n",
    "    return test_loss, accuracy, y_proba_out, y_truth_out\n",
    "\n",
    "# Evaluate model on test data\n",
    "loss, acc, y_proba, y_truth = overall_accuracy(model, test_loader, criterion)\n",
    "\n",
    "print(f\"Test Accuracy: {acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "069fa16dcc484b4ebed48cae6f7f8cbe": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1a16729ad428466c88a5b618b7e37062": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2949f13aeca94eda8555827eadc52fae",
      "max": 19,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_44a5ad178bfc4eee9e68535fd5bf5f8f",
      "value": 3
     }
    },
    "2949f13aeca94eda8555827eadc52fae": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2b0e82bbc5294756805dbe235a842ce0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_54b2a83b4f394e3bb6026b15b8def091",
      "placeholder": "",
      "style": "IPY_MODEL_b6df6d984d1c4c1689a899bd4e65bc8d",
      "value": "16%"
     }
    },
    "3ab3973edc4344deb89c236f119ee834": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "44a5ad178bfc4eee9e68535fd5bf5f8f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "54b2a83b4f394e3bb6026b15b8def091": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7b2ecfae6fbc4edcbf7673125b5ee836": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_2b0e82bbc5294756805dbe235a842ce0",
       "IPY_MODEL_1a16729ad428466c88a5b618b7e37062",
       "IPY_MODEL_e3fb1a1cb7a64e0ba2287a33f90930dd"
      ],
      "layout": "IPY_MODEL_069fa16dcc484b4ebed48cae6f7f8cbe"
     }
    },
    "7f0f91e9b1b44aae90a8001c22ce0553": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b6df6d984d1c4c1689a899bd4e65bc8d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "e3fb1a1cb7a64e0ba2287a33f90930dd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7f0f91e9b1b44aae90a8001c22ce0553",
      "placeholder": "",
      "style": "IPY_MODEL_3ab3973edc4344deb89c236f119ee834",
      "value": "3/19[02:39&lt;13:56,52.27s/it]"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
