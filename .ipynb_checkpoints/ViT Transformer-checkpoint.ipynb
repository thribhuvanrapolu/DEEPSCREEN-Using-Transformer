{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7fb31b17-f9d9-4ee2-bbc7-ab8e107b1e57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting vit_pytorch\n",
      "  Downloading vit_pytorch-1.6.9-py3-none-any.whl.metadata (65 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.7/65.7 kB\u001b[0m \u001b[31m850.0 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting einops>=0.7.0 (from vit_pytorch)\n",
      "  Downloading einops-0.8.0-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: torch>=1.10 in /dgxa_home/se21ucse231/anaconda3/lib/python3.11/site-packages (from vit_pytorch) (2.3.0)\n",
      "Requirement already satisfied: torchvision in /dgxa_home/se21ucse231/anaconda3/lib/python3.11/site-packages (from vit_pytorch) (0.18.0)\n",
      "Requirement already satisfied: filelock in /dgxa_home/se21ucse231/anaconda3/lib/python3.11/site-packages (from torch>=1.10->vit_pytorch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /dgxa_home/se21ucse231/anaconda3/lib/python3.11/site-packages (from torch>=1.10->vit_pytorch) (4.9.0)\n",
      "Requirement already satisfied: sympy in /dgxa_home/se21ucse231/anaconda3/lib/python3.11/site-packages (from torch>=1.10->vit_pytorch) (1.12)\n",
      "Requirement already satisfied: networkx in /dgxa_home/se21ucse231/anaconda3/lib/python3.11/site-packages (from torch>=1.10->vit_pytorch) (3.1)\n",
      "Requirement already satisfied: jinja2 in /dgxa_home/se21ucse231/anaconda3/lib/python3.11/site-packages (from torch>=1.10->vit_pytorch) (3.1.3)\n",
      "Requirement already satisfied: fsspec in /dgxa_home/se21ucse231/anaconda3/lib/python3.11/site-packages (from torch>=1.10->vit_pytorch) (2023.10.0)\n",
      "Requirement already satisfied: numpy in /dgxa_home/se21ucse231/anaconda3/lib/python3.11/site-packages (from torchvision->vit_pytorch) (1.26.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /dgxa_home/se21ucse231/anaconda3/lib/python3.11/site-packages (from torchvision->vit_pytorch) (10.2.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /dgxa_home/se21ucse231/anaconda3/lib/python3.11/site-packages (from jinja2->torch>=1.10->vit_pytorch) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in /dgxa_home/se21ucse231/anaconda3/lib/python3.11/site-packages (from sympy->torch>=1.10->vit_pytorch) (1.3.0)\n",
      "Downloading vit_pytorch-1.6.9-py3-none-any.whl (119 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m119.7/119.7 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading einops-0.8.0-py3-none-any.whl (43 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.2/43.2 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: einops, vit_pytorch\n",
      "Successfully installed einops-0.8.0 vit_pytorch-1.6.9\n"
     ]
    }
   ],
   "source": [
    "# !pip install linformer\n",
    "!pip install vit_pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f2125327-6930-41db-8fef-d54010c4bf80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from linformer import Linformer\n",
    "from PIL import Image\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from tqdm.notebook import tqdm\n",
    "from vit_pytorch.efficient import ViT\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import torch.utils.data as data\n",
    "import torchvision\n",
    "from torchvision.transforms import ToTensor\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3f22b222-89a8-4017-8ca2-e510480c7a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters:\n",
    "batch_size = 32\n",
    "epochs = 100\n",
    "lr = 0.001\n",
    "gamma = 0.7\n",
    "seed = 142\n",
    "IMG_SIZE = 200\n",
    "patch_size = 16\n",
    "num_classes = 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "78d64949-9dcf-49ee-88ea-4cb0444be6d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = torchvision.datasets.ImageFolder(\"training_files/target_training_datasets/CHEMBL286/dataset/training\", transform=ToTensor())\n",
    "valid_ds = torchvision.datasets.ImageFolder(\"training_files/target_training_datasets/CHEMBL286/dataset/test\", transform=ToTensor())\n",
    "test_ds = torchvision.datasets.ImageFolder(\"training_files/target_training_datasets/CHEMBL286/dataset/validation\", transform=ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "cb362226-fe77-47f5-a5bf-bdffc61eeb11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Loaders:\n",
    "train_loader = data.DataLoader(train_ds, batch_size=batch_size, shuffle=True,  num_workers=4)\n",
    "valid_loader = data.DataLoader(valid_ds, batch_size=batch_size, shuffle=True,  num_workers=4)\n",
    "test_loader  = data.DataLoader(test_ds, batch_size=batch_size, shuffle=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "1f749780-3578-4101-a3cb-64a2c2cfe562",
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "image dimensions must be divisible by the patch size",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[45], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m efficient_transformer \u001b[38;5;241m=\u001b[39m Linformer(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m128\u001b[39m, seq_len\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m64\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m, depth\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m12\u001b[39m, heads\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m, k\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m64\u001b[39m)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Vision Transformer Model: \u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m model \u001b[38;5;241m=\u001b[39m ViT(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m128\u001b[39m, image_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m128\u001b[39m, patch_size\u001b[38;5;241m=\u001b[39mpatch_size, num_classes\u001b[38;5;241m=\u001b[39mnum_classes, transformer\u001b[38;5;241m=\u001b[39mefficient_transformer, channels\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# loss function\u001b[39;00m\n\u001b[1;32m     11\u001b[0m criterion \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/vit_pytorch/efficient.py:13\u001b[0m, in \u001b[0;36mViT.__init__\u001b[0;34m(self, image_size, patch_size, num_classes, dim, transformer, pool, channels)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n\u001b[1;32m     12\u001b[0m image_size_h, image_size_w \u001b[38;5;241m=\u001b[39m pair(image_size)\n\u001b[0;32m---> 13\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m image_size_h \u001b[38;5;241m%\u001b[39m patch_size \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m image_size_w \u001b[38;5;241m%\u001b[39m patch_size \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimage dimensions must be divisible by the patch size\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m pool \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcls\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmean\u001b[39m\u001b[38;5;124m'\u001b[39m}, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpool type must be either cls (cls token) or mean (mean pooling)\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     15\u001b[0m num_patches \u001b[38;5;241m=\u001b[39m (image_size_h \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m patch_size) \u001b[38;5;241m*\u001b[39m (image_size_w \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m patch_size)\n",
      "\u001b[0;31mAssertionError\u001b[0m: image dimensions must be divisible by the patch size"
     ]
    }
   ],
   "source": [
    "# Training device:\n",
    "device = 'cuda'\n",
    "\n",
    "# Linear Transformer:\n",
    "efficient_transformer = Linformer(dim=128, seq_len=64+1, depth=12, heads=8, k=64)\n",
    "\n",
    "# Vision Transformer Model: \n",
    "model = ViT(dim=128, image_size=128, patch_size=patch_size, num_classes=num_classes, transformer=efficient_transformer, channels=3).to(device)\n",
    "\n",
    "# loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "# Learning Rate Scheduler for Optimizer:\n",
    "scheduler = StepLR(optimizer, step_size=1, gamma=gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "effab51a-c5ca-4b05-a4bf-84a0badcccbf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "864286576f2a467197e606d77e3ac55f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/37 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (2501) must match the size of tensor b (1025) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[37], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m data \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m      7\u001b[0m label \u001b[38;5;241m=\u001b[39m label\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m----> 9\u001b[0m output \u001b[38;5;241m=\u001b[39m model(data)\n\u001b[1;32m     10\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(output, label)\n\u001b[1;32m     12\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/vit_pytorch/efficient.py:43\u001b[0m, in \u001b[0;36mViT.forward\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     41\u001b[0m cls_tokens \u001b[38;5;241m=\u001b[39m repeat(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcls_token, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m() n d -> b n d\u001b[39m\u001b[38;5;124m'\u001b[39m, b \u001b[38;5;241m=\u001b[39m b)\n\u001b[1;32m     42\u001b[0m x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat((cls_tokens, x), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 43\u001b[0m x \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpos_embedding[:, :(n \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)]\n\u001b[1;32m     44\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformer(x)\n\u001b[1;32m     46\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mmean(dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpool \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmean\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m x[:, \u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (2501) must match the size of tensor b (1025) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "# Training:\n",
    "for epoch in range(epochs):\n",
    "    epoch_loss = 0\n",
    "    epoch_accuracy = 0\n",
    "    for data, label in tqdm(train_loader):\n",
    "        data = data.to(device)\n",
    "        label = label.to(device)\n",
    "\n",
    "        output = model(data)\n",
    "        loss = criterion(output, label)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        acc = (output.argmax(dim=1) == label).float().mean()\n",
    "        epoch_accuracy += acc / len(train_loader)\n",
    "        epoch_loss += loss / len(train_loader)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            epoch_val_accuracy = 0\n",
    "            epoch_val_loss = 0\n",
    "            \n",
    "        for data, label in valid_loader:\n",
    "            \n",
    "            data = data.to(device)\n",
    "            label = label.to(device)\n",
    "\n",
    "            val_output = model(data)\n",
    "            val_loss = criterion(val_output, label)\n",
    "\n",
    "            acc = (val_output.argmax(dim=1) == label).float().mean()\n",
    "            epoch_val_accuracy += acc / len(valid_loader)\n",
    "            epoch_val_loss += val_loss / len(valid_loader)\n",
    "\n",
    "    print(\n",
    "        f\"Epoch : {epoch+1} - loss : {epoch_loss:.4f} - acc: {epoch_accuracy:.4f} - val_loss : {epoch_val_loss:.4f} - val_acc: {epoch_val_accuracy:.4f}\\n\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b1557038-a5f9-4526-9e42-257a3e20df45",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'epochs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# load saved model:\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m PATH \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepochs\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;28mstr\u001b[39m(epochs)\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimg\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;28mstr\u001b[39m(IMG_SIZE)\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpatch\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;28mstr\u001b[39m(patch_size)\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;28mstr\u001b[39m(lr)\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.pt\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      3\u001b[0m efficient_transformer \u001b[38;5;241m=\u001b[39m Linformer(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m128\u001b[39m, seq_len\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m49\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m, depth\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m12\u001b[39m, heads\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m, k\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m64\u001b[39m)\n\u001b[1;32m      4\u001b[0m model \u001b[38;5;241m=\u001b[39m ViT(image_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m224\u001b[39m, patch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m, num_classes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m128\u001b[39m ,transformer\u001b[38;5;241m=\u001b[39mefficient_transformer, channels\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'epochs' is not defined"
     ]
    }
   ],
   "source": [
    "# load saved model:\n",
    "PATH = \"epochs\"+\"_\"+str(epochs)+\"_\"+\"img\"+\"_\"+str(IMG_SIZE)+\"_\"+\"patch\"+\"_\"+str(patch_size)+\"_\"+\"lr\"+\"_\"+str(lr)+\".pt\"\n",
    "efficient_transformer = Linformer(dim=128, seq_len=49+1, depth=12, heads=8, k=64)\n",
    "model = ViT(image_size=224, patch_size=32, num_classes=2, dim=128 ,transformer=efficient_transformer, channels=3)\n",
    "model.load_state_dict(torch.load(PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e1ca3153-e19f-47fd-baeb-dc6a608ed5b8",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 43\u001b[0m\n\u001b[1;32m     38\u001b[0m     y_truth_out \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([\u001b[38;5;28mfloat\u001b[39m(y_truth[i]) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(y_truth))])\n\u001b[1;32m     40\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m test_loss, accuracy, y_proba_out, y_truth_out\n\u001b[0;32m---> 43\u001b[0m loss, acc, y_proba, y_truth \u001b[38;5;241m=\u001b[39m overall_accuracy(model, test_loader, criterion \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss())\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAccuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00macc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28mprint\u001b[39m(pd\u001b[38;5;241m.\u001b[39mvalue_counts(y_truth))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "# Performance on Valid/Test Data\n",
    "def overall_accuracy(model, test_loader, criterion):\n",
    "    \n",
    "    '''\n",
    "    Model testing \n",
    "    \n",
    "    Args:\n",
    "        model: model used during training and validation\n",
    "        test_loader: data loader object containing testing data\n",
    "        criterion: loss function used\n",
    "    \n",
    "    Returns:\n",
    "        test_loss: calculated loss during testing\n",
    "        accuracy: calculated accuracy during testing\n",
    "        y_proba: predicted class probabilities\n",
    "        y_truth: ground truth of testing data\n",
    "    '''\n",
    "    \n",
    "    y_proba = []\n",
    "    y_truth = []\n",
    "    test_loss = 0\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    for data in tqdm(test_loader):\n",
    "        X, y = data[0].to('cpu'), data[1].to('cpu')\n",
    "        output = model(X)\n",
    "        test_loss += criterion(output, y.long()).item()\n",
    "        for index, i in enumerate(output):\n",
    "            y_proba.append(i[1])\n",
    "            y_truth.append(y[index])\n",
    "            if torch.argmax(i) == y[index]:\n",
    "                correct+=1\n",
    "            total+=1\n",
    "                \n",
    "    accuracy = correct/total\n",
    "    \n",
    "    y_proba_out = np.array([float(y_proba[i]) for i in range(len(y_proba))])\n",
    "    y_truth_out = np.array([float(y_truth[i]) for i in range(len(y_truth))])\n",
    "    \n",
    "    return test_loss, accuracy, y_proba_out, y_truth_out\n",
    "\n",
    "\n",
    "loss, acc, y_proba, y_truth = overall_accuracy(model, test_loader, criterion = nn.CrossEntropyLoss())\n",
    "\n",
    "\n",
    "print(f\"Accuracy: {acc}\")\n",
    "\n",
    "print(pd.value_counts(y_truth))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f62ed18e-256c-45c8-8b6c-0cd75de8f17f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
